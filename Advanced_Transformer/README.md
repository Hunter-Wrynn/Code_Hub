# Advanced Transformer

This directory contains implementations and notes on advanced transformer architectures for various applications. Each subdirectory focuses on a different type of transformer model with code implementations and documentation.

## Contents

- **Base_Transformer**  
  Basic transformer model implementation as a foundation for more complex variations.  
  **Paper:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- **Deformable_Transformer**  
  Implementation of deformable transformers, designed to handle variable-length sequences and dynamic structures in data.  
  **Paper:** [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159)

- **Dense_Prediction_Transformer**  
  A transformer model focused on dense prediction tasks, such as image segmentation and object detection.  
  **Paper:** [DPT: Dense Prediction Transformer](https://arxiv.org/abs/2103.13413)

- **DINO**  
  Implementation of DINO (Distillation with No Labels), a self-supervised learning approach based on transformers.  
  **Paper:** [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)

- **Llama**  
  Code for the LLaMA model (Large Language Model Meta AI), optimized for language-based tasks.  
  **Paper:** [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

- **Vision_Transformer**  
  Implementation of Vision Transformer (ViT) for image classification and other vision-related tasks.  
  **Paper:** [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

  - **Diffusion Transformer**  
  Implementation of Diffusion Transformer (DiT).  

## Usage

Each folder contains:
- The code implementation specific to each transformer model.
- Notes and documentation for understanding and using each model.

Refer to the individual directories for more details on each transformer type and its specific applications.
