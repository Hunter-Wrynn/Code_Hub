# Advanced Transformer

This directory contains implementations and notes on advanced transformer architectures for various applications. Each subdirectory focuses on a different type of transformer model with code implementations and documentation.

## Contents

- **Base_Transformer**  
  Basic transformer model implementation as a foundation for more complex variations.  
  **Paper:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- **CLIP**  
  Implementation of the CLIP (Contrastive Languageâ€“Image Pre-training) model, used for connecting vision and language tasks.  
  **Paper:** [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

- **DINO**  
  Implementation of DINO (Distillation with No Labels), a self-supervised learning approach based on transformers.  
  **Paper:** [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)

- **DPT (Dense Prediction Transformer)**  
  A transformer model focused on dense prediction tasks, such as image segmentation and object detection.  
  **Paper:** [DPT: Dense Prediction Transformer](https://arxiv.org/abs/2103.13413)

- **Deformable_Transformer**  
  Implementation of deformable transformers, designed to handle variable-length sequences and dynamic structures in data.  
  **Paper:** [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159)

- **DiT (Diffusion Transformer)**  
  Implementation of the Diffusion Transformer (DiT), designed for generative modeling and denoising tasks.  
  **Paper:** [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748)

- **LLAVA**  
  Implementation of Large Language and Vision Assistant (LLaVA) for multimodal conversational AI.  
  **Paper:** [LLaVA: Large Language and Vision Assistant](https://arxiv.org/abs/2305.11946)

- **Llama**  
  Code for the LLaMA model (Large Language Model Meta AI), optimized for language-based tasks.  
  **Paper:** [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

- **ViT (Vision Transformer)**  
  Implementation of Vision Transformer (ViT) for image classification and other vision-related tasks.  
  **Paper:** [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

## Usage

Each folder contains:
- The code implementation specific to each transformer model.
- Notes and documentation for understanding and using each model.

Refer to the individual directories for more details on each transformer type and its specific applications.
