{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大梦想!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 加载与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从HuggingFace加载，输入模型名称，即可加载对于的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 保存到本地\n",
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从本地加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer/\")\n",
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 查看词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##況': 16842,\n",
       " '##爐': 17314,\n",
       " 'ｔ': 8070,\n",
       " '漣': 4032,\n",
       " '##更': 16348,\n",
       " 'vision': 10762,\n",
       " '阎': 7330,\n",
       " '##垃': 14853,\n",
       " '##懶': 15811,\n",
       " '##渥': 17001,\n",
       " '##种': 17962,\n",
       " '##辊': 19836,\n",
       " '##掸': 16038,\n",
       " '鳩': 7853,\n",
       " '##host': 12227,\n",
       " '桨': 3444,\n",
       " '##」': 13655,\n",
       " '##拧': 15940,\n",
       " 'evernote': 13138,\n",
       " '蔑': 5912,\n",
       " '##均': 14829,\n",
       " '颡': 7586,\n",
       " '##韻': 20569,\n",
       " 'iv': 10573,\n",
       " 'も': 571,\n",
       " '##賂': 19590,\n",
       " '嗬': 1637,\n",
       " '籠': 5096,\n",
       " '殿': 3671,\n",
       " '伉': 822,\n",
       " '##{': 13344,\n",
       " '##ц': 13421,\n",
       " '##籌': 18149,\n",
       " '閡': 7282,\n",
       " '绾': 5343,\n",
       " '##lia': 10336,\n",
       " 'mems': 12530,\n",
       " '##満': 17027,\n",
       " 'vc': 9438,\n",
       " '##尘': 15269,\n",
       " '##賠': 19600,\n",
       " '##饋': 20694,\n",
       " '##駿': 20752,\n",
       " '軸': 6729,\n",
       " '##单': 14353,\n",
       " '台': 1378,\n",
       " '##な': 8730,\n",
       " '001': 9263,\n",
       " '381': 12185,\n",
       " '##唐': 14595,\n",
       " '##焚': 17247,\n",
       " '##钉': 20209,\n",
       " '３０': 10684,\n",
       " '##鹉': 20959,\n",
       " 'ж': 239,\n",
       " '100g': 10606,\n",
       " '##ena': 12000,\n",
       " '##虧': 19057,\n",
       " '卿': 1321,\n",
       " '壞': 1889,\n",
       " '懵': 2753,\n",
       " '↓↓': 12558,\n",
       " '##蕻': 19001,\n",
       " '贪': 6576,\n",
       " '瘢': 4605,\n",
       " '鹤': 7911,\n",
       " '1922': 10209,\n",
       " 'some': 13048,\n",
       " '##疮': 17612,\n",
       " '诏': 6405,\n",
       " '覇': 6209,\n",
       " '钼': 7184,\n",
       " '##覧': 19273,\n",
       " '雎': 7421,\n",
       " 'には': 8738,\n",
       " '390': 10370,\n",
       " '1944': 9462,\n",
       " '##oot': 13187,\n",
       " '##庆': 15469,\n",
       " '##乔': 13787,\n",
       " '##状': 17364,\n",
       " '奖': 1946,\n",
       " '任': 818,\n",
       " '##敬': 16200,\n",
       " '##豫': 19556,\n",
       " '咨': 1486,\n",
       " '聂': 5462,\n",
       " '雒': 7424,\n",
       " '绛': 5316,\n",
       " '碇': 4806,\n",
       " '##ak': 9896,\n",
       " 'lily': 11228,\n",
       " '##v': 8225,\n",
       " '##骨': 20812,\n",
       " 'china': 8873,\n",
       " '８': 8036,\n",
       " '1895': 10794,\n",
       " '##$': 13320,\n",
       " '賂': 6533,\n",
       " '紂': 5146,\n",
       " '##竭': 18055,\n",
       " '##芈': 18747,\n",
       " 'ヲ': 641,\n",
       " '蹈': 6688,\n",
       " '##苏': 18779,\n",
       " '##癱': 17686,\n",
       " '##劍': 14267,\n",
       " 'junior': 12552,\n",
       " '##襬': 19259,\n",
       " '衣': 6132,\n",
       " '防': 7344,\n",
       " '##贾': 19650,\n",
       " '椋': 3489,\n",
       " '1990': 8431,\n",
       " '##洙': 16877,\n",
       " '##邵': 19996,\n",
       " '##钛': 20217,\n",
       " '过': 6814,\n",
       " '##擠': 16146,\n",
       " '##16': 8518,\n",
       " 'd': 146,\n",
       " '##纍': 18323,\n",
       " '勋': 1237,\n",
       " 'sas': 12120,\n",
       " '觊': 6231,\n",
       " '雌': 7419,\n",
       " 'rose': 9497,\n",
       " '构': 3354,\n",
       " '##tical': 11912,\n",
       " 'isis': 11709,\n",
       " '##丢': 13753,\n",
       " 'meta': 11799,\n",
       " '鼐': 7960,\n",
       " '##17': 8408,\n",
       " 'lisa': 10032,\n",
       " '##垠': 14859,\n",
       " '栽': 3420,\n",
       " '瑄': 4440,\n",
       " '##免': 14105,\n",
       " '尽': 2226,\n",
       " '寧': 2180,\n",
       " '##癜': 17677,\n",
       " '##讶': 19442,\n",
       " '4l': 13297,\n",
       " '##噻': 14756,\n",
       " '##昂': 16260,\n",
       " '糠': 5137,\n",
       " '縄': 5234,\n",
       " 'ea': 9714,\n",
       " '##ini': 10489,\n",
       " '##蓄': 18955,\n",
       " '鄉': 6965,\n",
       " '##齿': 21033,\n",
       " '##薙': 19011,\n",
       " '##甫': 17559,\n",
       " '佔': 861,\n",
       " '缉': 5351,\n",
       " '##さ': 11647,\n",
       " '##2c': 11935,\n",
       " '呜': 1449,\n",
       " '##周': 14510,\n",
       " '##枇': 16412,\n",
       " '##菓': 18885,\n",
       " '##ode': 10260,\n",
       " '##kb': 10285,\n",
       " '吟': 1412,\n",
       " '##kin': 12018,\n",
       " '##薯': 19018,\n",
       " '##肥': 18560,\n",
       " '[unused35]': 35,\n",
       " '濡': 4091,\n",
       " '12306': 11914,\n",
       " '婢': 2043,\n",
       " '聊': 5464,\n",
       " '##传': 13894,\n",
       " '##憋': 15785,\n",
       " '##淇': 16956,\n",
       " '颌': 7571,\n",
       " 'q': 159,\n",
       " '贫': 6577,\n",
       " '##甯': 17562,\n",
       " '##菲': 18895,\n",
       " '賀': 6531,\n",
       " '##蛆': 19082,\n",
       " '##言': 19298,\n",
       " '##橼': 16645,\n",
       " '様': 3545,\n",
       " '##陪': 20430,\n",
       " '##are': 10305,\n",
       " '##ear': 11979,\n",
       " '##魍': 20850,\n",
       " '修': 934,\n",
       " '##瓶': 17543,\n",
       " '##窠': 18032,\n",
       " '2b': 10740,\n",
       " '##eg': 10935,\n",
       " '褫': 6193,\n",
       " '##拢': 15936,\n",
       " '##洁': 16872,\n",
       " '論': 6316,\n",
       " '##ik': 10445,\n",
       " '##佬': 13935,\n",
       " '##蹋': 19748,\n",
       " '##牢': 17343,\n",
       " '##鬧': 20842,\n",
       " '獲': 4363,\n",
       " '##拉': 15918,\n",
       " '墨': 1874,\n",
       " '30ml': 11234,\n",
       " '伢': 838,\n",
       " '腾': 5596,\n",
       " '訶': 6261,\n",
       " '98': 8327,\n",
       " '遁': 6875,\n",
       " '##ool': 11627,\n",
       " '##禅': 17940,\n",
       " '##怕': 15643,\n",
       " '##粱': 18175,\n",
       " '侧': 904,\n",
       " '##说': 19489,\n",
       " '葭': 5874,\n",
       " '麽': 7939,\n",
       " '##巒': 15389,\n",
       " '##戊': 15820,\n",
       " '莽': 5818,\n",
       " 'pass': 9703,\n",
       " '##規': 19268,\n",
       " '崗': 2305,\n",
       " '##銖': 20127,\n",
       " 'mhz': 12747,\n",
       " '##⁄': 13510,\n",
       " '##举': 13772,\n",
       " '##子': 15151,\n",
       " '歙': 3629,\n",
       " '畑': 4520,\n",
       " '863': 12874,\n",
       " '##菠': 18890,\n",
       " '##慄': 15761,\n",
       " '72': 8325,\n",
       " '售': 1545,\n",
       " 'ppi': 11393,\n",
       " '馄': 7665,\n",
       " '##a2': 12871,\n",
       " '##』': 13657,\n",
       " '～～': 8734,\n",
       " '。': 511,\n",
       " '##碉': 17864,\n",
       " 'rebecca': 10418,\n",
       " '##滋': 17053,\n",
       " '##撃': 16105,\n",
       " '##仟': 13863,\n",
       " '苻': 5742,\n",
       " '奔': 1944,\n",
       " '##瞓': 17795,\n",
       " '獄': 4352,\n",
       " '选': 6848,\n",
       " 'back': 10017,\n",
       " '##梁': 16505,\n",
       " '清': 3926,\n",
       " '胶': 5540,\n",
       " '##摀': 16084,\n",
       " '笏': 5009,\n",
       " 'dollars': 9448,\n",
       " '##荣': 18840,\n",
       " '##潤': 17113,\n",
       " '愈': 2689,\n",
       " ';': 132,\n",
       " '173': 10020,\n",
       " '玳': 4387,\n",
       " '##del': 13283,\n",
       " '##蚊': 19068,\n",
       " '泫': 3802,\n",
       " '譙': 6353,\n",
       " '骤': 7752,\n",
       " '##齣': 21031,\n",
       " '诲': 6431,\n",
       " '##翱': 18490,\n",
       " '1020': 13223,\n",
       " '蔣': 5919,\n",
       " '##住': 13914,\n",
       " '##♥': 9177,\n",
       " '##ssion': 12726,\n",
       " '##醜': 20068,\n",
       " '##嘣': 14720,\n",
       " '##尽': 15283,\n",
       " '眶': 4702,\n",
       " '溅': 3972,\n",
       " '嘟': 1661,\n",
       " '160': 8522,\n",
       " '##襁': 19254,\n",
       " '鄢': 6970,\n",
       " '❤2017': 8558,\n",
       " '##季': 15165,\n",
       " '##碴': 17881,\n",
       " '六': 1063,\n",
       " 'ferragamo': 9992,\n",
       " '##單': 14663,\n",
       " '##狙': 17376,\n",
       " '墊': 1865,\n",
       " '荨': 5787,\n",
       " '##蓝': 18962,\n",
       " '##叻': 14444,\n",
       " '沫': 3773,\n",
       " '##ート': 9006,\n",
       " '##气': 16755,\n",
       " '辣': 6793,\n",
       " '##埸': 14880,\n",
       " '汎': 3728,\n",
       " '勧': 1250,\n",
       " '##弊': 15521,\n",
       " '楝': 3505,\n",
       " '852': 11845,\n",
       " 'フ': 621,\n",
       " 'protected': 9562,\n",
       " '埗': 1817,\n",
       " '耐': 5447,\n",
       " '藕': 5969,\n",
       " '✦': 500,\n",
       " '潛': 4051,\n",
       " '骊': 7739,\n",
       " '貫': 6518,\n",
       " 'rdquo': 11071,\n",
       " '##壶': 14958,\n",
       " '##氮': 16770,\n",
       " '##隠': 20455,\n",
       " '鲑': 7829,\n",
       " '##„': 13497,\n",
       " '手': 2797,\n",
       " '垚': 1800,\n",
       " 'instagram': 8780,\n",
       " '##摟': 16095,\n",
       " '1917': 10199,\n",
       " '話': 6282,\n",
       " 'balance': 11606,\n",
       " '跚': 6653,\n",
       " '##苓': 18782,\n",
       " 'のないフロクに': 10900,\n",
       " '##飾': 20674,\n",
       " '##膩': 18668,\n",
       " '戴': 2785,\n",
       " '澀': 4066,\n",
       " '敝': 3138,\n",
       " 'sunday': 11548,\n",
       " '##泵': 16865,\n",
       " '擲': 3096,\n",
       " '燉': 4237,\n",
       " '绥': 5324,\n",
       " '真': 4696,\n",
       " '##lp': 10986,\n",
       " '401': 11185,\n",
       " 'memory': 11910,\n",
       " '##氧': 16766,\n",
       " '##泰': 16862,\n",
       " '##獭': 17418,\n",
       " '##細': 18226,\n",
       " '泣': 3798,\n",
       " '##辨': 19852,\n",
       " '##野': 20086,\n",
       " 'ㄆ': 648,\n",
       " 'key': 9232,\n",
       " '##缀': 18402,\n",
       " 'ヘ': 622,\n",
       " '泳': 3807,\n",
       " '違': 6889,\n",
       " '##懸': 15813,\n",
       " 'ᄁ': 289,\n",
       " '嗖': 1626,\n",
       " '##饿': 20719,\n",
       " '##google': 10626,\n",
       " '##bia': 12596,\n",
       " '##勋': 14294,\n",
       " 'local': 11476,\n",
       " '##洪': 16882,\n",
       " '##結': 18235,\n",
       " '##蛰': 19095,\n",
       " '##60': 8581,\n",
       " '##梭': 16517,\n",
       " 'png': 10246,\n",
       " '##ˊ': 13373,\n",
       " '183': 9826,\n",
       " 'tweet': 9874,\n",
       " '蝟': 6074,\n",
       " '327': 12094,\n",
       " '问': 7309,\n",
       " '##楂': 16557,\n",
       " '##2014': 8951,\n",
       " '##牺': 17352,\n",
       " '岬': 2274,\n",
       " '##61': 8978,\n",
       " '##癢': 17680,\n",
       " '森': 3481,\n",
       " '泷': 3809,\n",
       " '鞏': 7492,\n",
       " '##甾': 17572,\n",
       " '●': 474,\n",
       " '旁': 3178,\n",
       " '##倖': 14007,\n",
       " '##ince': 13199,\n",
       " 'rosie': 10487,\n",
       " '3kg': 9619,\n",
       " '譯': 6358,\n",
       " '700': 8389,\n",
       " '##贺': 19647,\n",
       " '卸': 1319,\n",
       " 'jean': 10483,\n",
       " '##ii': 8917,\n",
       " '##新': 16230,\n",
       " 'e2': 12357,\n",
       " '##睬': 17782,\n",
       " '啞': 1563,\n",
       " '媾': 2062,\n",
       " '躍': 6713,\n",
       " '227': 10543,\n",
       " '##脓': 18612,\n",
       " '##iner': 12045,\n",
       " '##碼': 17883,\n",
       " '總': 5244,\n",
       " '衾': 6143,\n",
       " '##正': 16690,\n",
       " '焖': 4186,\n",
       " '爍': 4256,\n",
       " '##椋': 16546,\n",
       " '庆': 2412,\n",
       " '願': 7544,\n",
       " '##負': 19568,\n",
       " '0020': 8746,\n",
       " '霏': 7454,\n",
       " '##崔': 15360,\n",
       " '豬': 6500,\n",
       " '摆': 3030,\n",
       " '明': 3209,\n",
       " '跎': 6650,\n",
       " '##符': 18073,\n",
       " '156': 9508,\n",
       " '辻': 6806,\n",
       " 'steven': 12174,\n",
       " '##鍾': 20166,\n",
       " '形': 2501,\n",
       " '棒': 3472,\n",
       " '补': 6133,\n",
       " '銬': 7073,\n",
       " '鋒': 7081,\n",
       " '授': 2956,\n",
       " '續': 5265,\n",
       " '赔': 6608,\n",
       " '##缕': 18412,\n",
       " '楔': 3503,\n",
       " 'basic': 12309,\n",
       " '##墳': 14934,\n",
       " '##幹': 15459,\n",
       " '漕': 4029,\n",
       " '芋': 5692,\n",
       " '膛': 5605,\n",
       " 'κ': 218,\n",
       " '侵': 909,\n",
       " '墮': 1876,\n",
       " '25000': 13251,\n",
       " '幾': 2407,\n",
       " '詹': 6285,\n",
       " '##偕': 14032,\n",
       " '##弗': 15529,\n",
       " '##郦': 20011,\n",
       " '飼': 7615,\n",
       " '##浃': 16895,\n",
       " '##驱': 20778,\n",
       " '##喉': 14647,\n",
       " '裘': 6169,\n",
       " '##暹': 16333,\n",
       " '##逻': 19929,\n",
       " '##缉': 18408,\n",
       " '暐': 3263,\n",
       " '鯖': 7807,\n",
       " '##閔': 20337,\n",
       " 'または': 12019,\n",
       " '##颡': 20643,\n",
       " '凛': 1123,\n",
       " 'agoda': 8678,\n",
       " '5mm': 9394,\n",
       " '##uk': 10687,\n",
       " '##⑷': 13569,\n",
       " '##窒': 18023,\n",
       " '386': 12303,\n",
       " '塔': 1849,\n",
       " '2m': 10725,\n",
       " '婵': 2049,\n",
       " '##桔': 16492,\n",
       " '##聆': 18520,\n",
       " '024': 13002,\n",
       " 'ヽ': 646,\n",
       " '慌': 2707,\n",
       " '暹': 3276,\n",
       " 'mon': 8556,\n",
       " '賊': 6538,\n",
       " 'ac': 9226,\n",
       " '##ald': 13041,\n",
       " '##袭': 19216,\n",
       " '♫': 494,\n",
       " '##魘': 20853,\n",
       " 'ⓒ': 424,\n",
       " '啜': 1562,\n",
       " '驗': 7710,\n",
       " '##or': 8372,\n",
       " '残': 3655,\n",
       " 'her': 11908,\n",
       " '躋': 6712,\n",
       " 'ipv6': 12082,\n",
       " '##屿': 15314,\n",
       " '##诅': 19455,\n",
       " '##研': 17834,\n",
       " '羽': 5417,\n",
       " '##py': 11610,\n",
       " '##噤': 14746,\n",
       " 'ク': 600,\n",
       " '##岱': 15333,\n",
       " '繞': 5254,\n",
       " 'wikia': 8708,\n",
       " '##佣': 13931,\n",
       " '##bug': 12351,\n",
       " 'english': 8899,\n",
       " '玩': 4381,\n",
       " '轰': 6764,\n",
       " '[unused12]': 12,\n",
       " 'beauty': 9445,\n",
       " '##兌': 14104,\n",
       " '##庙': 15479,\n",
       " '##狞': 17377,\n",
       " '记': 6381,\n",
       " '##燈': 17293,\n",
       " '##mah': 10394,\n",
       " '343': 12186,\n",
       " '##裁': 19218,\n",
       " '##遑': 19942,\n",
       " '咬': 1490,\n",
       " '憤': 2734,\n",
       " '鼹': 7964,\n",
       " '##呋': 14498,\n",
       " '##✕': 13634,\n",
       " '含': 1419,\n",
       " '##酰': 20052,\n",
       " 'fit': 11188,\n",
       " '##鐵': 20193,\n",
       " '##韆': 20556,\n",
       " '##黒': 21004,\n",
       " '##ｚ': 21100,\n",
       " '敢': 3140,\n",
       " 'pixstyleme3c': 8382,\n",
       " '詆': 6265,\n",
       " 'old': 10404,\n",
       " '##live': 11888,\n",
       " '苇': 5719,\n",
       " 'tai': 13242,\n",
       " '##綸': 18266,\n",
       " '[unused52]': 52,\n",
       " '316': 10743,\n",
       " '##浑': 16904,\n",
       " 'seo': 8475,\n",
       " '##vc': 12077,\n",
       " '##隅': 20440,\n",
       " '鳍': 7846,\n",
       " '##蹲': 19759,\n",
       " '拾': 2896,\n",
       " '帛': 2368,\n",
       " 'over': 10047,\n",
       " '##庇': 15470,\n",
       " '鰻': 7816,\n",
       " '##媲': 15116,\n",
       " '頌': 7520,\n",
       " '##蠶': 19171,\n",
       " 'icp': 9658,\n",
       " '##忘': 15620,\n",
       " '##环': 17441,\n",
       " '##遴': 19961,\n",
       " '##7': 8161,\n",
       " '##ale': 11079,\n",
       " '垒': 1799,\n",
       " '鹜': 7909,\n",
       " '##案': 16485,\n",
       " '267': 11253,\n",
       " '##妻': 15045,\n",
       " '圣': 1760,\n",
       " 'exhibition': 13282,\n",
       " '＊': 8022,\n",
       " '##李': 16387,\n",
       " '##お': 8345,\n",
       " '幄': 2387,\n",
       " '##76': 9624,\n",
       " '##www': 9718,\n",
       " '鲟': 7832,\n",
       " '##ｐ': 10748,\n",
       " '##粮': 18174,\n",
       " 'ω': 232,\n",
       " '##弥': 15534,\n",
       " '邨': 6931,\n",
       " 'mb': 9499,\n",
       " '銭': 7074,\n",
       " 'cf1': 12327,\n",
       " '砺': 4791,\n",
       " '##倏': 14003,\n",
       " '##⒊': 13573,\n",
       " 'select': 10634,\n",
       " 'namespace': 11076,\n",
       " '高': 7770,\n",
       " '嗣': 1632,\n",
       " '式': 2466,\n",
       " '##car': 9831,\n",
       " '282': 11431,\n",
       " '缽': 5376,\n",
       " '##噩': 14748,\n",
       " '茧': 5753,\n",
       " '##反': 14410,\n",
       " '##蒡': 18946,\n",
       " 'ccd': 12882,\n",
       " '匣': 1271,\n",
       " '215': 10082,\n",
       " '##①': 13556,\n",
       " '##尴': 15276,\n",
       " '##琊': 17475,\n",
       " 'ちてすか': 9880,\n",
       " 'nb': 9254,\n",
       " '酉': 6977,\n",
       " '##啾': 14640,\n",
       " '鹞': 7910,\n",
       " '##啊': 14614,\n",
       " '##逃': 19902,\n",
       " '狎': 4315,\n",
       " '鈣': 7048,\n",
       " '##®': 8646,\n",
       " 'cheese': 11387,\n",
       " '##碚': 17872,\n",
       " 'ab': 9386,\n",
       " '##量': 20087,\n",
       " 'なのて': 10925,\n",
       " '爲': 4264,\n",
       " '產': 4496,\n",
       " '##赔': 19665,\n",
       " '犧': 4304,\n",
       " '##500': 9758,\n",
       " '##毙': 16744,\n",
       " '胖': 5523,\n",
       " 'ⓔ': 425,\n",
       " '##戎': 15823,\n",
       " '##zone': 11004,\n",
       " '吠': 1413,\n",
       " '萸': 5860,\n",
       " '##か': 8261,\n",
       " '##临': 13764,\n",
       " 'abs': 8494,\n",
       " '蚯': 6021,\n",
       " '##剪': 14255,\n",
       " '獗': 4357,\n",
       " '##粲': 18176,\n",
       " '##兩': 14117,\n",
       " '##篑': 18122,\n",
       " 'maker': 11389,\n",
       " '驟': 7713,\n",
       " '乾': 746,\n",
       " '儘': 1029,\n",
       " '##尼': 15282,\n",
       " '##恒': 15665,\n",
       " '锆': 7223,\n",
       " '##aw': 10922,\n",
       " '骡': 7751,\n",
       " '弃': 2461,\n",
       " '越': 6632,\n",
       " '##☼': 13623,\n",
       " '##哲': 14585,\n",
       " '蜷': 6063,\n",
       " '##bb': 10214,\n",
       " '##醪': 20071,\n",
       " '仃': 786,\n",
       " '塵': 1859,\n",
       " '##迪': 19889,\n",
       " 'entertainment': 11679,\n",
       " '惕': 2664,\n",
       " '##黎': 21001,\n",
       " '##俠': 13984,\n",
       " '##擱': 16152,\n",
       " '慧': 2716,\n",
       " 'ᅲ': 317,\n",
       " '剣': 1193,\n",
       " 'power': 9239,\n",
       " '堅': 1830,\n",
       " 'oculus': 10480,\n",
       " 'adobe': 8864,\n",
       " '淬': 3916,\n",
       " '##孱': 15173,\n",
       " '##酒': 20040,\n",
       " 'anna': 9779,\n",
       " '##鈎': 20101,\n",
       " '1894': 12221,\n",
       " '3s': 13247,\n",
       " '##阳': 20402,\n",
       " '纭': 5282,\n",
       " '##鲸': 20898,\n",
       " '坎': 1775,\n",
       " '並': 699,\n",
       " '##廿': 15514,\n",
       " '猪': 4343,\n",
       " '箱': 5056,\n",
       " '##舶': 18724,\n",
       " 'mp': 11198,\n",
       " '##蒙': 18942,\n",
       " '##诊': 19459,\n",
       " '##袋': 19207,\n",
       " '獵': 4364,\n",
       " '##矽': 17826,\n",
       " '##ell': 9602,\n",
       " '##豢': 19554,\n",
       " '西': 6205,\n",
       " '##绻': 18397,\n",
       " '烨': 4174,\n",
       " '##ょう': 11663,\n",
       " '棺': 3486,\n",
       " '##奋': 14996,\n",
       " '##沂': 16809,\n",
       " 'write': 12800,\n",
       " '##謙': 19397,\n",
       " '##梓': 16509,\n",
       " '瀕': 4106,\n",
       " '勸': 1253,\n",
       " '##天': 14978,\n",
       " '##履': 15309,\n",
       " '肱': 5508,\n",
       " '##ko': 8827,\n",
       " '##極': 16570,\n",
       " '##饱': 20710,\n",
       " '】': 524,\n",
       " '##需': 20501,\n",
       " '##锋': 20283,\n",
       " 'lohasthree': 11741,\n",
       " '##より': 10190,\n",
       " '曳': 3290,\n",
       " '##叮': 14433,\n",
       " '##減': 16995,\n",
       " '##醛': 20067,\n",
       " '繕': 5252,\n",
       " '134': 9146,\n",
       " 'twd600': 11230,\n",
       " '##劈': 14264,\n",
       " 'kb': 9843,\n",
       " '##呲': 14512,\n",
       " '##寡': 15233,\n",
       " '##漬': 17093,\n",
       " 'tue': 8483,\n",
       " '##xp': 11379,\n",
       " '##诣': 19476,\n",
       " '##ーション': 12301,\n",
       " 'john': 8675,\n",
       " '##沥': 16826,\n",
       " 'スホンサー': 10948,\n",
       " '##愜': 15753,\n",
       " '##桀': 16479,\n",
       " '##ience': 13225,\n",
       " '##碑': 17868,\n",
       " 'e1': 12298,\n",
       " '岐': 2262,\n",
       " '##蓓': 18960,\n",
       " '##禁': 17938,\n",
       " '##訣': 19311,\n",
       " '##吧': 14473,\n",
       " '##皂': 17694,\n",
       " '錠': 7091,\n",
       " '##邮': 19991,\n",
       " '##※': 13508,\n",
       " 'より': 10230,\n",
       " '霭': 7461,\n",
       " '俠': 927,\n",
       " '##勖': 14298,\n",
       " '⑧': 412,\n",
       " '##蠍': 19162,\n",
       " 'jr': 8602,\n",
       " '06': 8116,\n",
       " '##嘶': 14731,\n",
       " '##鄉': 20022,\n",
       " 'uber': 8624,\n",
       " '##览': 19286,\n",
       " 'last': 10148,\n",
       " '##腭': 18642,\n",
       " '##怔': 15642,\n",
       " '債': 1002,\n",
       " '##炼': 17216,\n",
       " '汇': 3726,\n",
       " '##耶': 18513,\n",
       " 'unit': 12816,\n",
       " '##韋': 20557,\n",
       " '蟹': 6101,\n",
       " '##裳': 19235,\n",
       " '##孪': 15169,\n",
       " '铎': 7195,\n",
       " '吓': 1405,\n",
       " '后': 1400,\n",
       " '舀': 5641,\n",
       " '矾': 4770,\n",
       " '豆': 6486,\n",
       " '野': 7029,\n",
       " '##赚': 19668,\n",
       " 'has': 11325,\n",
       " '##骈': 20795,\n",
       " '歐': 3627,\n",
       " '##失': 14984,\n",
       " 'im': 10481,\n",
       " '溺': 3989,\n",
       " '##ul': 10086,\n",
       " '##down': 11140,\n",
       " '偽': 984,\n",
       " '孳': 2117,\n",
       " '##揶': 16059,\n",
       " '##宽': 15217,\n",
       " '##¤': 13349,\n",
       " '⑥': 410,\n",
       " '##堯': 14896,\n",
       " '##嚀': 14758,\n",
       " '##ung': 9112,\n",
       " '##遣': 19954,\n",
       " '釣': 7037,\n",
       " '衔': 6124,\n",
       " '##ㄇ': 13707,\n",
       " '帶': 2380,\n",
       " 'linux': 8403,\n",
       " 'here': 10815,\n",
       " '##檻': 16658,\n",
       " 'sui': 10997,\n",
       " '巨': 2342,\n",
       " '##刀': 14200,\n",
       " '呈': 1439,\n",
       " '##源': 17032,\n",
       " '##濟': 17146,\n",
       " 'icecat': 9336,\n",
       " '##蓼': 18967,\n",
       " 'plan': 12248,\n",
       " '##撓': 16112,\n",
       " '抬': 2848,\n",
       " '骰': 7757,\n",
       " '瀅': 4100,\n",
       " '##cast': 13020,\n",
       " '害': 2154,\n",
       " '##辇': 19833,\n",
       " '疇': 4539,\n",
       " 'android': 8254,\n",
       " '##攒': 16161,\n",
       " '潘': 4050,\n",
       " '##疗': 17602,\n",
       " '##ع': 13434,\n",
       " '##同': 14455,\n",
       " '憲': 2740,\n",
       " '##阴': 20403,\n",
       " '宕': 2133,\n",
       " '旬': 3194,\n",
       " '攢': 3111,\n",
       " '##«': 13352,\n",
       " '##箭': 18112,\n",
       " '580': 10298,\n",
       " '澡': 4074,\n",
       " '##や': 9164,\n",
       " 'cto': 12633,\n",
       " '佛': 867,\n",
       " '幽': 2406,\n",
       " '[unused64]': 64,\n",
       " '##佯': 13936,\n",
       " '啻': 1581,\n",
       " '簾': 5088,\n",
       " '##枰': 16427,\n",
       " '##钒': 20213,\n",
       " '##铂': 20246,\n",
       " '##掐': 16017,\n",
       " '布': 2357,\n",
       " '铺': 7215,\n",
       " '##茎': 18806,\n",
       " '秧': 4913,\n",
       " '##np': 12798,\n",
       " 'oppo': 8806,\n",
       " '##职': 18523,\n",
       " '杷': 3349,\n",
       " '蝈': 6067,\n",
       " '##錮': 20153,\n",
       " '##絶': 18247,\n",
       " 'ユ': 632,\n",
       " '##lab': 11532,\n",
       " '尔': 2209,\n",
       " '##髒': 20823,\n",
       " '##ray': 10009,\n",
       " '##stry': 11001,\n",
       " 'follow': 10792,\n",
       " '##駝': 20749,\n",
       " '##慶': 15780,\n",
       " '##追': 19898,\n",
       " '##net': 8914,\n",
       " '##牠': 17341,\n",
       " '##閹': 20347,\n",
       " 's3': 10647,\n",
       " 'bye': 10833,\n",
       " '##檸': 16657,\n",
       " '柏': 3377,\n",
       " '##tal': 9917,\n",
       " '##king': 9740,\n",
       " '漉': 4025,\n",
       " 'pinterest': 8379,\n",
       " '蹇': 6687,\n",
       " '##驊': 20763,\n",
       " '##醃': 20059,\n",
       " 'wear': 12679,\n",
       " '晗': 3240,\n",
       " '歇': 3623,\n",
       " '谢': 6468,\n",
       " '▲topdec': 10499,\n",
       " '邓': 6924,\n",
       " '##厮': 14397,\n",
       " '##疔': 17600,\n",
       " '##石': 17824,\n",
       " 'ᅨ': 310,\n",
       " '心': 2552,\n",
       " '##林': 16417,\n",
       " '##罂': 18434,\n",
       " '##ᅣ': 13471,\n",
       " '##艹': 18742,\n",
       " '斡': 3164,\n",
       " '##蕃': 18988,\n",
       " '桡': 3438,\n",
       " '##蘸': 19042,\n",
       " '五': 758,\n",
       " '炉': 4140,\n",
       " '缚': 5359,\n",
       " '##颗': 20635,\n",
       " '##论': 19446,\n",
       " '##able': 9609,\n",
       " 'リストに': 11101,\n",
       " '摩': 3040,\n",
       " '##吒': 14461,\n",
       " '円': 1080,\n",
       " '##睹': 17783,\n",
       " '##cal': 10384,\n",
       " 'js': 9016,\n",
       " '##克': 14103,\n",
       " '跻': 6668,\n",
       " '诘': 6409,\n",
       " '##т': 13417,\n",
       " '渥': 3944,\n",
       " '##类': 18159,\n",
       " '焯': 4194,\n",
       " '##攤': 16170,\n",
       " '##ルハイト': 13304,\n",
       " '跹': 6666,\n",
       " '##艳': 18740,\n",
       " '井': 759,\n",
       " '哗': 1517,\n",
       " '∟': 382,\n",
       " '##椰': 16553,\n",
       " '##组': 18356,\n",
       " '．': 8026,\n",
       " '實': 2179,\n",
       " '866': 11029,\n",
       " '##闕': 20356,\n",
       " '##§': 13351,\n",
       " '95': 8287,\n",
       " '##珮': 17466,\n",
       " '筋': 5025,\n",
       " '岳': 2277,\n",
       " '谱': 6480,\n",
       " '##am': 8608,\n",
       " '##メ': 10597,\n",
       " '│': 429,\n",
       " '瀬': 4115,\n",
       " '##农': 14150,\n",
       " '##悌': 15692,\n",
       " '锟': 7233,\n",
       " '氧': 3709,\n",
       " '立': 4989,\n",
       " 'bill': 10681,\n",
       " '##藻': 19033,\n",
       " '馳': 7682,\n",
       " '皂': 4637,\n",
       " '抡': 2842,\n",
       " '##兰': 14122,\n",
       " '凍': 1120,\n",
       " '厨': 1337,\n",
       " '##睏': 17768,\n",
       " '影': 2512,\n",
       " 'double': 11255,\n",
       " '咄': 1466,\n",
       " '##婚': 15099,\n",
       " '劫': 1223,\n",
       " 'al': 9266,\n",
       " '##灼': 17190,\n",
       " 'a5': 10796,\n",
       " '##則': 14236,\n",
       " '慾': 2725,\n",
       " 'ap': 9392,\n",
       " '〝': 531,\n",
       " 'ᅪ': 312,\n",
       " '##镖': 20316,\n",
       " '##th': 8414,\n",
       " '##∠': 13535,\n",
       " '拌': 2863,\n",
       " '恸': 2627,\n",
       " '##イン': 10108,\n",
       " '##蚵': 19080,\n",
       " 'น': 278,\n",
       " '##筑': 18086,\n",
       " '##｛': 21101,\n",
       " 'copy': 11669,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 索引转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  更便捷的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(sen, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 梦 想! [SEP]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 填充与截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 其他输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx != 0 else 0 for idx in ids]\n",
    "token_type_ids = [0] * len(ids)\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 快速调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102], [101, 3300, 3457, 2682, 6443, 6963, 749, 679, 6629, 102], [101, 6841, 6852, 3457, 2682, 4638, 2552, 8024, 3683, 3457, 2682, 3315, 6716, 8024, 3291, 1377, 6586, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = [\"弱小的我也有大梦想\",\n",
    "        \"有梦想谁都了不起\",\n",
    "        \"追逐梦想的心，比梦想本身，更可贵\"]\n",
    "res = tokenizer(sens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 73.3 ms, sys: 661 µs, total: 73.9 ms\n",
      "Wall time: 73.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 139 ms, sys: 16.3 ms, total: 156 ms\n",
      "Wall time: 13.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast / Slow Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大Dreaming!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\", use_fast=False)\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 656 ms, sys: 80 µs, total: 656 ms\n",
      "Wall time: 656 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 0 ns, total: 1.82 s\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 924 ms, sys: 107 ms, total: 1.03 s\n",
      "Wall time: 191 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = fast_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.68 s, sys: 0 ns, total: 1.68 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = slow_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mslow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmadp3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2864\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2862\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2863\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2864\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/llmadp3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2974\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2953\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2954\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2971\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2972\u001b[0m     )\n\u001b[1;32m   2973\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmadp3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3050\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3040\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3041\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3042\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3043\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3047\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3048\u001b[0m )\n\u001b[0;32m-> 3050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3053\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmadp3/lib/python3.9/site-packages/transformers/tokenization_utils.py:793\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    789\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    790\u001b[0m             )\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore information on available tokenizers at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    799\u001b[0m     )\n\u001b[1;32m    801\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    802\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sen, return_offsets_mapping=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特殊Tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://127.0.0.1:7890\"\n",
    "os.environ['https_proxy'] = \"http://127.0.0.1:7890\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5f310b7dc84ea5a31ee00dad6b80b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdd9fa2cc4f4a29807880c4401f7621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_skywork.py:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Skywork/Skywork-13B-base:\n",
      "- tokenization_skywork.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6faeee42e17e469f856b496f99606bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/994k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3332c0d57948039ab4473420380b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers_modules.Skywork.Skywork-13B-base.bc35915066fbbf15b77a1a4a74e9b574ab167816.tokenization_skywork.SkyworkTokenizer'>. This means that tokens that come after special tokens will not be properly handled. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SkyworkTokenizer(name_or_path='Skywork/Skywork-13B-base', vocab_size=65519, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新版本的transformers（>4.34），加载 THUDM/chatglm 会报错，因此这里替换为了天宫的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-13B-base\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"skywork_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"skywork_tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>弱小的我也有大Dreaming!'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmadp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
