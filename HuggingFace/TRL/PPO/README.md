# ğŸ¤– Hugging Face `.generate()` è¾“å‡ºç»“æ„ä¸æ‰‹åŠ¨æ¥åˆçš„å¿…è¦æ€§

## ä¸€ã€èƒŒæ™¯ç®€ä»‹

ä½¿ç”¨ Hugging Face Transformers ä¸­çš„ `model.generate()` ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œå¸¸è§çš„è¿”å›ç»“æ„ï¼š

```python
output = model.generate(
    input_ids=queries,
    attention_mask=attention_mask,
    generation_config=generation_config,
    return_dict_in_generate=True,
    output_scores=True,
)
```

è¿”å›çš„ `output` åŒ…æ‹¬ï¼š

* `output.sequences`: åŒ…å« `[prompt + response]` çš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º `[batch_size, total_seq_len]`
* `output.scores`: æ¯ä¸€æ­¥ç”Ÿæˆ token çš„ logitsï¼Œåˆ—è¡¨å½¢å¼ï¼Œé•¿åº¦ä¸º `gen_len`

---

## äºŒã€ä¸ºä»€ä¹ˆä¸ç›´æ¥ä½¿ç”¨ `output.sequences`

### 1. `.generate()` å¯èƒ½æ’å…¥ç‰¹æ®Š token

* åŒ…æ‹¬ `[PAD]`, `<|endoftext|>` ç­‰ç‰¹æ®Šæ ‡è®°
* æ¨¡å‹å¯èƒ½å¯¹è¾“å…¥åš left-padding æˆ– truncationï¼Œå¯¼è‡´ prompt ä½ç½®åç§»

### 2. `output.scores` ä¸ `sequences` é•¿åº¦ä¸ä¸€è‡´

* `output.scores` ä»…è®°å½• **ç”Ÿæˆéƒ¨åˆ†** çš„ logits
* è‹¥ä¸æ‰‹åŠ¨åˆ†ç¦» `response`ï¼Œéš¾ä»¥å‡†ç¡®å¯¹é½ logits ä¸ token

### 3. éš¾ä»¥åˆ†å¼€å¤„ç† prompt / response

* å¤šæ•°è®­ç»ƒæ–¹æ³•ï¼ˆSFTã€PPOã€DPOã€RMï¼‰éœ€è¦åˆ†åˆ«å¤„ç† prompt ä¸ response
* ç›´æ¥ä½¿ç”¨ `output.sequences` ä¼šå¯¼è‡´è¾¹ç•Œä¸æ¸…æ™°

---

## ä¸‰ã€æ¨èçš„æ‰‹åŠ¨æ¥åˆæµç¨‹

### âœ… æ­¥éª¤ 1ï¼šè°ƒç”¨ `.generate()`

```python
output = model.generate(
    input_ids=queries,
    attention_mask=(queries != tokenizer.pad_token_id),
    generation_config=generation_config,
    return_dict_in_generate=True,
    output_scores=True,
)
```

### âœ… æ­¥éª¤ 2ï¼šæ‰‹åŠ¨æå– response

```python
context_len = queries.shape[1]
response = output.sequences[:, context_len:]
full_sequence = torch.cat((queries, response), dim=1)
```

### âœ… æ­¥éª¤ 3ï¼šæå– logits å¹¶å¯¹é½ token

```python
logits = torch.stack(output.scores, dim=1)  # [batch, gen_len, vocab_size]
logprobs = torch.nn.functional.log_softmax(logits, dim=-1)
logprobs_for_tokens = logprobs.gather(-1, response.unsqueeze(-1)).squeeze(-1)
```

### âœ… æ­¥éª¤ 4ï¼šå¯é€‰è§£ç ï¼ˆä¾¿äºè°ƒè¯•ï¼‰

```python
decoded_prompt = tokenizer.batch_decode(queries, skip_special_tokens=True)
decoded_response = tokenizer.batch_decode(response, skip_special_tokens=True)
decoded_full = tokenizer.batch_decode(full_sequence, skip_special_tokens=True)
```

---

## å››ã€æ¨èä½¿ç”¨æƒ…å½¢æ€»ç»“

| åœºæ™¯                  | æ˜¯å¦æ¨èæ‰‹åŠ¨æ¥åˆ                   | ç†ç”± |
| ------------------- | -------------------------- | -- |
| æ¨ç†é˜¶æ®µï¼Œæ— éœ€å¯¹é½ logits    | âŒ å¯ç›´æ¥ä½¿ç”¨ `output.sequences` |    |
| RLHF / PPO / DPO è®­ç»ƒ | âœ…âœ… å¼ºçƒˆæ¨èæ‰‹åŠ¨åˆ†å‰²ä¸æ‹¼æ¥             |    |
| éœ€è¦æ‰“åˆ†ã€è¯„ä»·å“åº”è´¨é‡         | âœ… æ¨èæ‹†åˆ†åç‹¬ç«‹å¤„ç†                |    |

---

# ğŸš€ é«˜æ•ˆæŸ¥æ‰¾å¼ é‡ä¸­ç¬¬ä¸€ä¸ª True çš„ç´¢å¼•

## ä¸€ã€åº”ç”¨èƒŒæ™¯

åœ¨è®­ç»ƒä¸­ç»å¸¸éœ€è¦ï¼š

* æ‰¾åˆ°åºåˆ—ä¸­ç¬¬ä¸€ä¸ª padding ä½ç½®
* åˆ¤æ–­ response æœ‰æ•ˆ token çš„èŒƒå›´
* é¿å…ä½¿ç”¨ Python å¾ªç¯ï¼Œæé«˜æ•ˆç‡

---

## äºŒã€æ¨èå‡½æ•°ï¼š`first_true_indices`

```python
def first_true_indices(bools: torch.Tensor, dtype=torch.long) -> torch.Tensor:
    row_len = bools.size(-1)
    zero_or_index = row_len * (~bools).type(dtype) + torch.arange(row_len, dtype=dtype, device=bools.device)
    return torch.min(zero_or_index, dim=-1).values
```

---

## ä¸‰ã€å·¥ä½œåŸç†è¯¦è§£

1. `~bools`: å°† True å˜ä¸º Falseï¼ŒFalse å˜ä¸º True
2. `~bools * row_len`: é True ä½ç½®èµ‹å€¼ä¸€ä¸ªè¾ƒå¤§çš„æ•°å­—ï¼ˆæ— æ•ˆï¼‰
3. `+ torch.arange(...)`: åŠ ä¸Šå½“å‰ç´¢å¼•
4. `min(...)`: æ‰¾å‡ºæ¯è¡Œæœ€å°å€¼ï¼Œå³ç¬¬ä¸€ä¸ª True çš„ç´¢å¼•

---

## å››ã€å‡½æ•°ä¼˜åŠ¿

| ä¼˜åŠ¿       | è¯´æ˜                            |
| -------- | ----------------------------- |
| ğŸš€ é«˜æ€§èƒ½   | å®Œå…¨çŸ¢é‡åŒ–ï¼Œé€‚ç”¨äº GPU è¿è¡Œ              |
| ğŸ§  ä¼˜é›…ç®€æ´  | æ— éœ€ Python å¾ªç¯ï¼Œä»£ç çŸ­å°æ˜“æ‡‚           |
| âš¡ è‡ªåŠ¨è¾¹ç•Œå¤„ç† | è‹¥æ²¡æœ‰ Trueï¼Œè¿”å›è¡Œé•¿åº¦ï¼ˆè¡¨ç¤ºæ— æ•ˆï¼‰          |
| âœ… æ”¯æŒæ¢¯åº¦   | å¯åµŒå…¥ differentiable pipeline ä¸­ |

---

## äº”ã€ä¸æ¨èå†™æ³•

```python
# å­˜åœ¨æ€§èƒ½é—®é¢˜ä¸å®‰å…¨éšæ‚£ï¼š
for row in bools:
    index = (row == True).nonzero()[0]  # è‹¥æ²¡æœ‰ True ä¼šæŠ›å¼‚å¸¸
```

---

## å…­ã€å®é™…ç¤ºä¾‹ï¼šç”¨äº PPO ä¸­è·å–å“åº”é•¿åº¦

```python
response_mask = (response == pad_token_id)
valid_response_lengths = first_true_indices(response_mask) - 1
```

è¯¥é€»è¾‘ç”¨äºåˆ¤æ–­ç”Ÿæˆå“åº”ä¸­æœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„ä½ç½®ã€‚

---

## ä¸ƒã€é€‚ç”¨åœºæ™¯æ€»ç»“

| ç›®æ ‡                             | æ¨èæ–¹æ³•                      |
| ------------------------------ | ------------------------- |
| è·å–å“åº” token æœ‰æ•ˆé•¿åº¦                | ä½¿ç”¨ `first_true_indices()` |
| æ›¿ä»£ for å¾ªç¯æœç´¢ True               | ä½¿ç”¨çŸ¢é‡åŒ–å®ç°                   |
| PPO / DPO / RLHF ä¸­ response æˆªæ–­ | é…åˆ `pad_token_id` åˆ¤æ–­      |


# ğŸ¯ PPO ä¸­ä¸ºä½•åˆ†åˆ«ä½¿ç”¨ `query_response` å’Œ `postprocessed_query_response`

åœ¨ PPOï¼ˆProximal Policy Optimizationï¼‰è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­ï¼Œå¯¹åŒä¸€ä¸ªæ¨¡å‹è¾“å‡ºï¼ˆresponseï¼‰ï¼Œæˆ‘ä»¬ä¼šæ„é€ ä¸¤ä¸ªä¸åŒç‰ˆæœ¬çš„è¾“å…¥ï¼š

## ğŸ“¦ å˜é‡åŒºåˆ«

| å˜é‡å                            | å«ä¹‰                      | æ˜¯å¦æˆªæ–­                   | ç”¨é€”                               |
| ------------------------------ | ----------------------- | ---------------------- | -------------------------------- |
| `query_response`               | åŸå§‹çš„ `query + response`  | å¦                      | ç”¨äº value modelï¼ˆä¼°è®¡æ¯ä¸ª token çš„çŠ¶æ€ä»·å€¼ï¼‰ |
| `postprocessed_query_response` | æˆªæ–­åçš„ `query + response` | æ˜¯ï¼ˆæˆªæ–­äº `stop_token_id`ï¼‰ | ç”¨äº reward modelï¼ˆå¯¹æœ€ç»ˆ response æ‰“åˆ†ï¼‰ |

---

## ğŸ§  ä¸ºä»€ä¹ˆç”¨ä¸¤ä¸ªä¸åŒç‰ˆæœ¬ï¼Ÿ

### âœ… Value Model ä½¿ç”¨ `query_response`

* value model éœ€è¦ä¼°è®¡æ¯ä¸ª token çš„ "æœªæ¥å›æŠ¥"ï¼ˆVï¼‰
* æ‰€ä»¥å¿…é¡»çœ‹åˆ° **å®Œæ•´çš„åºåˆ—**ï¼ˆåŒ…æ‹¬ stop token åçš„ tokenï¼‰
* ç”¨äºè®¡ç®— PPO çš„ advantageï¼š

```math
A_t = R_t - V(s_t)
```

---

### âœ… Reward Model ä½¿ç”¨ `postprocessed_query_response`

* reward model ç”¨äºç»™æ•´ä¸ª response æ‰“ä¸€ä¸ªæ€»ä½“åˆ†æ•°ï¼ˆRï¼‰
* response ä¸­ stop token ä¹‹åçš„ token å¾€å¾€æ˜¯æ¨¡å‹â€œä¹±ç”Ÿæˆâ€çš„å†…å®¹ï¼Œå¦‚åºŸè¯ã€é‡å¤ç­‰
* æ‰€ä»¥å¿…é¡» **æˆªæ–­è‡³ stop\_token\_id**ï¼Œç¡®ä¿ reward ä¸å—æ— æ•ˆ token å¹²æ‰°

---

## ğŸ”„ è°ƒç”¨ç¤ºä¾‹

```python
# full value estimation from entire response
full_value, _, _ = get_reward(value_model, query_response, pad_token_id, context_length)
value = full_value[:, context_length-1:-1].squeeze(-1)  # for PPO advantage

# final scalar reward from cleaned response
_, score, _ = get_reward(reward_model, postprocessed_query_response, pad_token_id, context_length)
```

---

## âœ… æ€»ç»“

> ä½¿ç”¨ä¸¤ä¸ªç‰ˆæœ¬ï¼Œæ˜¯ä¸ºäº†ç¡®ä¿ï¼š
>
> * value model çš„è¾“å…¥ **ä¿¡æ¯å®Œæ•´**ï¼Œç”¨äºå­¦ä¹ ä»·å€¼ä¼°è®¡
> * reward model çš„è¾“å…¥ **è¯­ä¹‰å¹²å‡€**ï¼Œé¿å…è¯„åˆ†å—åˆ°å¹²æ‰°

è¿™ç§è¾“å…¥åˆ†ç¦»ï¼Œæ˜¯ RLHF/PPO å®è·µä¸­çš„é‡è¦å·¥ç¨‹ä¼˜åŒ–ç­–ç•¥ã€‚
