# ğŸ¤– ä¸ºä»€ä¹ˆä¸ç›´æ¥ä½¿ç”¨ `output.sequences` ï¼Œè€Œæ˜¯æ‰‹åŠ¨æ¥åˆ `queries + response`

å½“ä½ ä½¿ç”¨ Hugging Face çš„ `.generate()` æ–¹æ³•æ—¶ï¼Œé€šå¸¸ä¼šå¾—åˆ°ï¼š

```python
output = model.generate(
    input_ids=queries,
    attention_mask=attention_mask,
    generation_config=generation_config,
    return_dict_in_generate=True,
    output_scores=True,
)
```

è¿™ä¸ª `output` åŒ…æ‹¬ï¼š

```python
output.sequences  # Tensorï¼Œå½¢çŠ¶ [batch_size, total_seq_len]
output.scores     # List[Tensor]ï¼Œæ¯æ­¥ç”Ÿæˆçš„ logits
```

è™½ç„¶ `output.sequences` çœ‹èµ·æ¥åŒ…å«äº† `[prompt + response]`ï¼Œä½†åœ¨ RLHF / PPO è®­ç»ƒåœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸è¿˜æ˜¯é€‰æ‹©æ‰‹åŠ¨æ¥åˆï¼š

```python
context_len = queries.shape[1]
response = output.sequences[:, context_len:]
full_sequence = torch.cat((queries, response), dim=1)
```

---

## âœ… æ˜¯å¦å¿…é¡»æ‰‹åŠ¨æ¥åˆï¼Ÿ

> ä¸æ˜¯å¿…é¡»ï¼Œä½†åœ¨ PPO / RLHF ç­‰éœ€è¦å®æ—¶ç»Ÿè®¡ / ä¸¥æ ¼å¯¹é½çš„åœºæ™¯ä¸­ï¼Œåº”å½“ä¼˜å…ˆè€ƒè™‘æ‰‹åŠ¨æ¥åˆã€‚

---

## ğŸ“‰ è¯¦ç»†åˆ†æ

### 1. æ˜ç¡®æ§åˆ¶ç»“æ„

`.generate()` æœ‰æ—¶ä¼šï¼š
- æ’å…¥ special tokenï¼ˆå¦‚ `[PAD]`, `<|endoftext|>`ï¼‰
- å›  `max_length` æˆªæ–­ prompt
- ä½¿ç”¨ left-padding å¯¼è‡´ prompt ä½ç½®åç§»

è€Œæ‰‹åŠ¨æ¥åˆï¼Œä¿è¯ç»“æ„æ˜¯ï¼š

```python
[prompt (queries)] + [response (generated)]
```

---

### 2. ä¸ `output.scores` å¯¹é½

```python
output.scores  # æ¯ä¸€æ­¥ç”Ÿæˆçš„ logits
```

å®ƒçš„é•¿åº¦ = ç”Ÿæˆ token çš„ä¸ªæ•°ï¼Œè€Œä¸æ˜¯ `output.sequences.shape[1]`ï¼Œå› ä¸ºåè€…è¿˜åŒ…å« prompt

æ‰€ä»¥ï¼Œåªæœ‰å°† response åˆ†å‰²å‡ºæ¥ï¼Œæ‰èƒ½å¯¹é½ logits

```python
context_len = queries.shape[1]
response = output.sequences[:, context_len:]
logits = torch.stack(output.scores, dim=1)  # [batch_size, gen_len, vocab_size]
```

---

### 3. ä¾¿äºåˆ†å¼€ prompt å’Œ response ç”¨äºè¯„åˆ†

å¾ˆå¤šè®­ç»ƒåœºæ™¯ï¼ˆå¦‚ Reward Modelï¼ŒSFTï¼ŒDPOï¼ŒPPOï¼‰ï¼Œéœ€è¦åˆ†åˆ«å¯¹ prompt å’Œ response è¿›è¡Œå¤„ç†

æ‰‹åŠ¨æ¥åˆ / åˆ†å‰²èƒ½æ›´ç²¾ç¡®åœ°ç®¡ç†æ¯ä¸ªéƒ¨åˆ†

---

## âœ… æ¨èä»£ç 

```python
# 1. ç”Ÿæˆè¾“å‡º
output = model.generate(
    input_ids=queries,
    attention_mask=(queries != tokenizer.pad_token_id),
    generation_config=generation_config,
    return_dict_in_generate=True,
    output_scores=True,
)

# 2. åˆ†å‰² prompt å’Œ response
context_len = queries.shape[1]
response = output.sequences[:, context_len:]
full_sequence = torch.cat((queries, response), dim=1)

# 3. å¤„ç† logits
logits = torch.stack(output.scores, dim=1)  # [batch_size, gen_len, vocab_size]
logprobs = torch.nn.functional.log_softmax(logits, dim=-1)
logprobs_for_tokens = logprobs.gather(-1, response.unsqueeze(-1)).squeeze(-1)

# 4. å¯é€‰: è§£ç ä¸ºæ–‡æœ¬
decoded_prompt = tokenizer.batch_decode(queries, skip_special_tokens=True)
decoded_response = tokenizer.batch_decode(response, skip_special_tokens=True)
decoded_full = tokenizer.batch_decode(full_sequence, skip_special_tokens=True)
```

---

## âš ï¸ ç›´æ¥ä½¿ç”¨ `output.sequences` å¯èƒ½å¯¼è‡´é—®é¢˜

| é—®é¢˜ | è¯´æ˜ |
|--------|------|
| prompt è¢«æˆªæ–­/å¡‘é€  | `.generate()` ä¼šè‡ªåŠ¨å¤„ç† input |
| logits å’Œ token å¯¹ä¸ä¸Š | `output.scores` åªæ˜¯ response éƒ¨åˆ† |
| prompt/response åˆ†ç•Œä¸æ¸…æ¥š | éš¾ä»¥ç”¨äºåˆ†ç±»/è¯„åˆ† |

---

## âœ… æœ€ä½³å®è·µ

| ç”¨æ³• | æ˜¯å¦æ¨è | ç†ç”± |
|--------|---------|------|
| ç›´æ¥ä½¿ç”¨ `output.sequences` | âœ… å¯ä»¥ | å½“ç¡®å®šç»“æ„æ­£ç¡®æ—¶ |
| æ‰‹åŠ¨æ¥åˆ `queries + response` | âœ…âœ… æ¨è | ç»“æ„æ˜ç¡®ï¼Œå®‰å…¨ä¸”æ˜“äºè°ƒè¯• |

---
